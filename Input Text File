Since you have a Huffman tree in the form of a single HuffNode, you also have all of the Huffman codes you will need to compress the body of the file. Rather than querying the tree and extracting a code for every character in the file, it will be much more efficient to pre-process your tree and extract all of the codes before compressing the body of the file. As with the frequencies, you can either use an array or a Map to store the new codes. Populating your array/Map can be done with a basic recursive traversal. At the base case where you reach a leaf, simply add a new entry to the array or Map. Otherwise make a recursive call down the left and right sub-tree with the correct updated arguments. Although in theory it does not matter whether 0 indicates the left or right, our implementation uses 0 to indicate the left and 1 to indicate the right, so in order to match our process exactly and optimize your score for Huffman, please use the same pattern. It is advised that you store eachtFirst, check to make sure the HUFF_NUMBER is present at the beginning of the file using the BitInputStream and the readBits() method. You should read BITS_PER_INT bits. If the HUFF_NUMBER isn't there, throw a HuffException with an informative message. Once that is verified, you can move on to reading the header with a recursive helper method. First, read a single bit. If the bit is a 0, then you are at an internal node and need to make two recursive calls to build the current node's two sub-trees. Be sure to make the recursive calls in the right order so that your codes aren't flipped. If the bit was instead a 1, then you need to read the next 9 bits to get the value of the character. Create a new HuffNode with the value of the 9 bits you just read and weight of whatever you'd like. The weight doesn't matter now since the entire structure of the tree is specified by the header. If you wrote the header correctly, then this recursive process should terminate naturally at exactly the end of the header. If instead you are encountering errors, check the write and read methods for the header for any minor mistakes.With a tree, all you have to do now is continue to decode characters until you reach the PSEUDO_EOF character. With a while loop, read one bit at a time. Depending on the value of that bit, move the current node to either the left or right sub-child. Include a if-case check for the bit equaling -1, if a file was not compressed properly or there's an error in your decompress methods, then you may miss the PSEUDO_EOF and read all the way to the full end of the file. If the current bit equals -1, throw a HuffException like before with the message "Problem with the PSEUDO_EOF" so that you will know where to begin debugging if the error should occur. If the bit was valid and you move to a new node, check if it is a leaf. If so, then check if the value of the node is the PSEUDO_EOF. When you encounter the PSEUDO_EOF, you need to return so that you stop writing and reading bits. For all other characters, write BITS_PER_WORD bits using the BitOutputStream with the value of the character stored in the node. After writing the character, you also need to reset the current node pointer to the root of the tree. By now you should have a fully working compress and decompress system which you can verify using the compare tab. Once you are sure that your code works, move onto the analysis or check out the extra credit section.Since Huffman coding relies on character entropy/frequencies to generate new codes, the first step of your compression algorithm should be to count the number of occurrences of each character in the file. You can read bits from the provided BitInputStream using readBits(int howManyBits). You should read BITS_PER_WORD (which equals 8) bits at a time in order to get a single character at each call. You should continue to call in.readBits(BITS_PER_WORD) until it returns -1, which indicates that you've reached the end of the file. Storing the frequencies is best done using either an array or a Map. Since Java can automatically convert char and int primitive values, you can use the indexes of an array as a faster alternative to Character keys in a Map, although both data structures are appropriate. Either way, the values should hold the accumulated frequency of each character. Be sure to reset your BitInputStream once you have finished reading all of the characters so that you can reread the file without error when you encode it later. You can do this by calling in.reset().

Now that you know how often each character occurs, you can construct a Huffman tree based on the file. To construct the tree, first create a HuffNode for each character that occurs at least once. You don't want to include characters that don't occur at all since this will make your tree much larger than it needs to be and minimize the effectiveness of your compression algorithm. Remember also that the whole point of Huffman coding is to leave out the characters that don't occur at all. The weight of each HuffNode should be the character's frequency and the value should be the character itself, converted automatically by Java to an int. Remember to include the pseudo-eof character in the tree; its HuffNode should have a weight of 0 and a value of PSEUDO_EOF, a constant in Processor. You'll need to put each HuffNode into a data structure of your choosing. Since HuffNode implements Comparable, you could use either an array with Arrays.sort(HuffNode[] nodes), some type of List with Collections.sort(List nodes), or any other built in Java library you please.

However, the classic solution to Huffman is to use a PriorityQueue which will automatically sort the nodes for you and return the smallest weighted node each time you call poll(). This implementation is much faster than using an array or a List since you don't have to do a full sort at each step. No matter which data structure you use, you'll need to use a while loop to continue combining nodes until you only have one left. To combine two nodes, first remove the two smallest nodes from your data structure. Then create a new node with the two nodes as its sub-children and their combined weight as its weight. Put the new node back into the data structure and repeat. The node that remains at the end is the rNow that you've seen an example, try creating a Huffman tree for yourself. Use "go go gophers" as the source for the tree so you can check your answer with the image above. WARNING: when you're done, your tree may not exactly match the picture. This is ok; your characters don't have to be in the same spots, and the structure of the tree may be slightly different. That said, you should end up with something that has essentially the same shape as the example, perhaps with some left and right sub-children flipped. If you aren't certain that your tree is correct, try again, selecting nodes and orientations to specifically emulate the example.

Before you begin to actually implement Huffman, here are some interesting dilemmas to consider. First, compression isn't free. Your decompression algorithm needs to be able to reconstruct the tree you used to compress the file in order to decompress it. You'll have to include some information at the beginning of each of your compressed files which instructs the decompress method on how to recreate the specific tree that it needs. There are a number of ways to encode this information, but they all require a similar amount of space. This additional information or header is why many shorter files can't be compressed; the additional length of the header outweighs the number of bits saved by compressing the body of the file. Second, since the decompress algorithm requires very specific formatting in order to work, inputting a file that was never compressed in the first place could yield some interesting results. In order to prevent decompress from trying to parse nonsensical information, you'll need to include some sort of flag or indicator at the beginning of your compressed files as well to communicate to decompress that it is all right to proceed. There is a constant provided for you in Processor that you can use for this purpose called HUFF_NUMBER. It is highly unlikely that HUFF_NUMBER will naturally occur at the beginning of the file, so this is a nearly 100% effective approach. Finally, since all of the Huffman bit codes are of variable length, there is no surefire mathematical way to know when you've reached the end of the file and need to stop reading bits. Although the file does eventually end, there may be some number of extra bits at the end since files are written in bytes, not bits, and so every file must have a length with a multiple of 8 bits. This could result in additional characters being added to the end of your decompressed file which did not exist in the original uncompressed version. To combat this issue, you should include an end of file character at the end of each of your compressed files. The pseudo-EOF will have its own Huffman code generated from the tree. The decompression algorithm won't write this character to your decompressed file, but rather, once it encounters the unique pseudo-EOF code, it will trigger your loop to stop reading bits and exit.oot of your Huffman tree.you've reached the end of the file and need to stop reading bits. Although the file does eventually end, there may be some number of extra bits at the end since files are written in bytes, not bits, and so every file must have a length with a multiple of 8 bits. This could result in additional characters being added to the end of your decompressed file which diduse the same strategy to build your Huffman tree as this may be an unnecessary source of confusion. Additionally, it is highly recommended that you write a number of helper methods rather than one long method to implement compress. It is better design and will be much easier to debug should you enchave finished readi